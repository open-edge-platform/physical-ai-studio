{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75744671",
   "metadata": {},
   "source": [
    "## 1. Using the physicalai Python API\n",
    "\n",
    "The most direct way to use Groot is through the `physicalai` API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b692b8",
   "metadata": {},
   "source": [
    "### 1.1 Basic Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc2988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (C) 2025 Intel Corporation\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "from physicalai.policies.groot import Groot\n",
    "\n",
    "Groot??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Groot()\n",
    "\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad1ab5c",
   "metadata": {},
   "source": [
    "### 1.2 Training with physicalai Trainer\n",
    "\n",
    "The `physicalai.train.Trainer` is a Lightning subclass with conveniences for robotics training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe23c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physicalai.data.lerobot import LeRobotDataModule\n",
    "from physicalai.policies.groot import Groot\n",
    "from physicalai.train import Trainer\n",
    "\n",
    "# Create policy (24GB GPU settings)\n",
    "policy = Groot(\n",
    "    base_model_path=\"nvidia/GR00T-N1.5-3B\",\n",
    "    embodiment_tag=\"new_embodiment\",\n",
    "    chunk_size=50,\n",
    "    n_action_steps=50,\n",
    "    tune_projector=True,\n",
    "    tune_diffusion_model=False,  # Set True for 48GB+ GPUs\n",
    ")\n",
    "\n",
    "# Create data module\n",
    "datamodule = LeRobotDataModule(\n",
    "    repo_id=\"lerobot/aloha_sim_transfer_cube_human\",\n",
    "    data_format=\"lerobot\",\n",
    "    train_batch_size=1,  # Use 1 for 24GB GPUs\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"gpu\",  # Use \"xpu\" for Intel GPUs\n",
    "    devices=1,\n",
    "    precision=\"bf16-mixed\",\n",
    "    accumulate_grad_batches=16,  # Effective batch size = 1 x 16 = 16\n",
    "    gradient_clip_val=1.0,\n",
    "    log_every_n_steps=10,\n",
    "    fast_dev_run=True,  # Set to False to run full training\n",
    ")\n",
    "\n",
    "# To train, uncomment the following line:\n",
    "# >>> trainer.fit(policy, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4703f650",
   "metadata": {},
   "source": [
    "### 1.3 Inference with Groot\n",
    "\n",
    "After training, use the policy for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0fe319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Prepare observation (example with dummy data)\n",
    "# In practice, this comes from your robot's sensors\n",
    "batch = {\n",
    "    # Images: (B, C, H, W) - batch, channels, height, width\n",
    "    \"observation.images.cam_high\": torch.randn(1, 3, 224, 224),\n",
    "    \"observation.images.cam_left_wrist\": torch.randn(1, 3, 224, 224),\n",
    "    \"observation.images.cam_right_wrist\": torch.randn(1, 3, 224, 224),\n",
    "    # Robot state: (B, state_dim)\n",
    "    \"observation.state\": torch.randn(1, 14),\n",
    "    # Task instruction\n",
    "    \"task\": \"Pick up the red cube and place it on the blue plate\",\n",
    "}\n",
    "\n",
    "# Run inference (policy handles device placement automatically)\n",
    "# >>> policy.eval()\n",
    "# >>> with torch.no_grad():\n",
    "# >>>     actions = policy.select_action(batch)\n",
    "# >>> actions.shape  # (B, action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d46a63",
   "metadata": {},
   "source": [
    "## 2. Using HuggingFace Configuration\n",
    "\n",
    "You can also load Groot using HuggingFace's configuration system. This is useful when you want to use the same configuration format as the original NVIDIA release."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7da845",
   "metadata": {},
   "source": [
    "### 2.2 Creating GrootModel from HuggingFace Pretrained\n",
    "\n",
    "The underlying `GrootModel` uses the same architecture as NVIDIA's implementation but with PyTorch native attention (SDPA) for broader hardware support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a74286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physicalai.policies.groot.model import GrootModel\n",
    "\n",
    "# Load GrootModel from HuggingFace pretrained weights\n",
    "# This automatically downloads and loads the NVIDIA weights\n",
    "model = GrootModel.from_pretrained(\n",
    "    \"nvidia/GR00T-N1.5-3B\",\n",
    "    attn_implementation=\"sdpa\",  # Use PyTorch native attention\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb33837",
   "metadata": {},
   "source": [
    "### 2.3 Wrapping GrootModel in Lightning Module\n",
    "\n",
    "For training, wrap the model in the `Groot` Lightning module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physicalai.policies.groot import Groot\n",
    "\n",
    "# Create Groot policy - it handles model loading internally\n",
    "policy = Groot(\n",
    "    base_model_path=\"nvidia/GR00T-N1.5-3B\",\n",
    "    embodiment_tag=\"new_embodiment\",\n",
    "    attn_implementation=\"sdpa\",  # Use PyTorch native attention\n",
    ")\n",
    "\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40c00b7",
   "metadata": {},
   "source": [
    "### 2.4 Using HuggingFace Datasets\n",
    "\n",
    "The `LeRobotDataModule` can load datasets directly from HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physicalai.data.lerobot import LeRobotDataModule\n",
    "\n",
    "# Available datasets on HuggingFace Hub (examples)\n",
    "datasets = [\n",
    "    \"lerobot/aloha_sim_transfer_cube_human\",\n",
    "    \"lerobot/aloha_sim_insertion_human\",\n",
    "    \"lerobot/pusht\",\n",
    "    \"lerobot/xarm_lift_medium\",\n",
    "]\n",
    "\n",
    "# Load a dataset\n",
    "datamodule = LeRobotDataModule(\n",
    "    repo_id=\"lerobot/aloha_sim_transfer_cube_human\",\n",
    "    data_format=\"lerobot\",\n",
    "    train_batch_size=1,\n",
    ")\n",
    "\n",
    "# Setup and inspect\n",
    "datamodule.setup(\"fit\")\n",
    "len(datamodule.train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc208d2",
   "metadata": {},
   "source": [
    "## 3. Using the CLI\n",
    "\n",
    "The `physicalai` CLI provides a convenient way to train models using YAML configuration files.\n",
    "\n",
    "### Basic Usage (24GB GPUs)\n",
    "\n",
    "```bash\n",
    "physicalai fit --config configs/physicalai/groot.yaml\n",
    "```\n",
    "\n",
    "### Intel XPU Support\n",
    "\n",
    "```bash\n",
    "physicalai fit --config configs/physicalai/groot.yaml --trainer.accelerator xpu\n",
    "```\n",
    "\n",
    "### GPU-Specific Overrides\n",
    "\n",
    "**48GB GPU** (A6000, L40):\n",
    "```bash\n",
    "physicalai fit --config configs/physicalai/groot.yaml \\\n",
    "    --model.tune_diffusion_model true \\\n",
    "    --data.train_batch_size 2 \\\n",
    "    --trainer.accumulate_grad_batches 8\n",
    "```\n",
    "\n",
    "**80GB GPU** (A100, H100):\n",
    "```bash\n",
    "physicalai fit --config configs/physicalai/groot.yaml \\\n",
    "    --model.tune_diffusion_model true \\\n",
    "    --data.train_batch_size 4 \\\n",
    "    --trainer.accumulate_grad_batches 4\n",
    "```\n",
    "\n",
    "### Using a Different Dataset\n",
    "\n",
    "```bash\n",
    "physicalai fit --config configs/physicalai/groot.yaml --data.repo_id lerobot/pusht\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd6476",
   "metadata": {},
   "source": [
    "## 4. Memory Requirements Summary\n",
    "\n",
    "| Configuration | Trainable Params | VRAM Required |\n",
    "|--------------|------------------|---------------|\n",
    "| `tune_projector` only (default) | ~518M | 24GB |\n",
    "| `tune_projector` + `tune_diffusion_model` | ~1.1B | 48GB |\n",
    "| + `tune_llm` or `tune_visual` | ~2.7B+ | 80GB+ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e42cd",
   "metadata": {},
   "source": [
    "## 5. Tips and Best Practices\n",
    "\n",
    "### Hardware Selection\n",
    "- **24GB GPUs** (3090, 4090, Intel B580): Use default settings (`tune_projector=True`, `tune_diffusion_model=False`)\n",
    "- **48GB+ GPUs**: Enable `tune_diffusion_model=True` for better results\n",
    "- **Intel XPU**: Use `attn_implementation=\"sdpa\"` (default) - works natively\n",
    "\n",
    "### Training Tips\n",
    "- Start with the default configuration and adjust based on loss curves\n",
    "- Use gradient accumulation to achieve larger effective batch sizes\n",
    "\n",
    "### Attention Implementation\n",
    "- `sdpa`: PyTorch native, works on CUDA and XPU (default)\n",
    "- `flash_attention_2`: Requires CUDA and flash-attn package\n",
    "- `eager`: Fallback, slower but most compatible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c5a8c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geti-action",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
