{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBERO Complete Walkthrough: Gym & Benchmark\n",
    "\n",
    "This notebook provides a **comprehensive guide** to working with LIBERO in PhysicalAI:\n",
    "\n",
    "**Part 1: Understanding LIBERO**\n",
    "- What is LIBERO and its task suites\n",
    "- Installation and setup\n",
    "\n",
    "**Part 2: LiberoGym (Low-Level)**\n",
    "- Creating environments\n",
    "- Observation and action spaces\n",
    "- Running single rollouts\n",
    "\n",
    "**Part 3: Policy Creation**\n",
    "- First-party ACT policy\n",
    "- Third-party LeRobot policies (Diffusion, VQ-BeT)\n",
    "- Loading from datasets\n",
    "\n",
    "**Part 4: Evaluation Tools**\n",
    "- `rollout()` - Single episode evaluation\n",
    "- `Rollout` metric - TorchMetrics for Lightning\n",
    "- `evaluate_policy()` - Multi-episode evaluation\n",
    "\n",
    "**Part 5: LiberoBenchmark (High-Level)**\n",
    "- Benchmark class for standardized evaluation\n",
    "- Multi-task evaluation\n",
    "- Results export (JSON, CSV)\n",
    "- Video recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from physicalai.gyms.libero import LiberoGym\n",
    "\n",
    "# PhysicalAI imports\n",
    "from physicalai.data import Feature, FeatureType, NormalizationParameters\n",
    "from physicalai.devices import get_available_device\n",
    "from physicalai.policies import ACT, ACTModel\n",
    "\n",
    "# Check device (supports CUDA, XPU, and CPU)\n",
    "device = get_available_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LIBERO Benchmark Overview\n",
    "\n",
    "| Suite | Tasks | Max Steps | Focus |\n",
    "|-------|-------|-----------|-------|\n",
    "| `libero_spatial` | 10 | 280 | Spatial reasoning (same objects, different positions) |\n",
    "| `libero_object` | 10 | 280 | Object generalization (different objects, same actions) |\n",
    "| `libero_goal` | 10 | 300 | Goal-conditioned tasks (same scene, different goals) |\n",
    "| `libero_10` | 10 | 520 | Mixed difficulty benchmark |\n",
    "| `libero_90` | 90 | 400 | Large-scale comprehensive benchmark |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from libero.libero import benchmark\n",
    "except ModuleNotFoundError:\n",
    "    msg = \"LIBERO is not installed. Install it with:\\n  pip install hf-libero\\nOr with uv:\\n  uv pip install hf-libero\"\n",
    "    raise ImportError(msg) from None\n",
    "\n",
    "# List available tasks\n",
    "for suite_name in [\"libero_spatial\", \"libero_object\"]:\n",
    "    suite = benchmark.get_benchmark_dict()[suite_name]()\n",
    "    tasks = suite.get_task_names()\n",
    "    print(f\"\\n{suite_name} ({len(tasks)} tasks):\")\n",
    "    for i, task in enumerate(tasks[:3]):\n",
    "        print(f\"  [{i}] {task}\")\n",
    "    if len(tasks) > 3:\n",
    "        print(f\"  ... and {len(tasks) - 3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create LiberoGym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LiberoGym instance\n",
    "gym = LiberoGym(\n",
    "    task_suite=\"libero_spatial\",\n",
    "    task_id=0,\n",
    "    observation_height=256,\n",
    "    observation_width=256,\n",
    "    obs_type=\"pixels_agent_pos\",  # Images + proprioception\n",
    "    control_mode=\"relative\",  # Delta actions\n",
    ")\n",
    "\n",
    "print(f\"Task: {gym.task_name}\")\n",
    "print(f\"Max episode steps: {gym.max_episode_steps}\")\n",
    "print(f\"Action space: {gym.action_space.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset and inspect observation\n",
    "obs, info = gym.reset(seed=42)\n",
    "\n",
    "print(f\"Observation type: {type(obs).__name__}\")\n",
    "print(f\"\\nImages: {list(obs.images.keys())}\")\n",
    "for name, img in obs.images.items():\n",
    "    print(f\"  {name}: {img.shape}\")\n",
    "\n",
    "print(f\"\\nState: {obs.state.shape}\")\n",
    "print(\"  Format: [eef_pos(3), axis_angle(3), gripper(2)]\")\n",
    "print(f\"  Values: {obs.state.squeeze().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize camera views\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "img1 = obs.images[\"image\"].squeeze(0).permute(1, 2, 0).numpy()\n",
    "img2 = obs.images[\"image2\"].squeeze(0).permute(1, 2, 0).numpy()\n",
    "\n",
    "axes[0].imshow(img1)\n",
    "axes[0].set_title(\"Front Camera (agentview)\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(img2)\n",
    "axes[1].set_title(\"Eye-in-Hand Camera\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Task: {gym.task_name[:50]}...\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Policy Features (Manual Approach)\n",
    "\n",
    "PhysicalAI uses `Feature` dataclasses to define input/output shapes. This is the **manual approach** - you can also use `ACT.from_dataset()` for automatic feature extraction (see Section 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features matching LiberoGym output (with normalization for ACT)\n",
    "input_features = {\n",
    "    \"image\": Feature(\n",
    "        ftype=FeatureType.VISUAL,\n",
    "        shape=(3, 256, 256),\n",
    "        normalization_data=NormalizationParameters(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ),\n",
    "    \"image2\": Feature(\n",
    "        ftype=FeatureType.VISUAL,\n",
    "        shape=(3, 256, 256),\n",
    "        normalization_data=NormalizationParameters(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ),\n",
    "    \"state\": Feature(\n",
    "        ftype=FeatureType.STATE,\n",
    "        shape=(8,),\n",
    "        normalization_data=NormalizationParameters(mean=[0.0] * 8, std=[1.0] * 8),\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Output features (7-dim action)\n",
    "output_features = {\n",
    "    \"action\": Feature(\n",
    "        ftype=FeatureType.ACTION,\n",
    "        shape=(7,),\n",
    "        normalization_data=NormalizationParameters(mean=[0.0] * 7, std=[1.0] * 7),\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"Input features:\")\n",
    "for name, feat in input_features.items():\n",
    "    print(f\"  {name}: {feat.ftype.name} {feat.shape}\")\n",
    "print(f\"\\nOutput: action {output_features['action'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create First-Party ACT Policy\n",
    "\n",
    "Two approaches to create an ACT policy:\n",
    "1. **Manual**: Define features yourself (shown above)\n",
    "2. **From Dataset**: Use `ACT.from_dataset()` for automatic feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Manual feature definition (from Section 4)\n",
    "act_model = ACTModel(\n",
    "    input_features=input_features,\n",
    "    output_features=output_features,\n",
    "    chunk_size=100,\n",
    "    dim_model=256,\n",
    "    n_encoder_layers=2,\n",
    "    n_decoder_layers=1,\n",
    ")\n",
    "act_policy = ACT(model=act_model)\n",
    "act_policy.to(device)\n",
    "act_policy.eval()\n",
    "\n",
    "print(f\"âœ… First-party ACT policy created on {device}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in act_policy.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Using `ACT.from_dataset()`\n",
    "\n",
    "This approach automatically extracts features from a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Create ACT from a dataset (automatic feature extraction)\n",
    "from physicalai.data.lerobot import LeRobotDataModule\n",
    "\n",
    "# Create datamodule from a compatible LeRobot dataset\n",
    "# For LIBERO, use \"lerobot/libero_spatial\" once migrated to v3.0 format\n",
    "datamodule = LeRobotDataModule(\n",
    "    repo_id=\"lerobot/aloha_sim_transfer_cube_human\",  # Example with ALOHA (2 arms, 4 cameras)\n",
    "    train_batch_size=32,\n",
    "    data_format=\"physicalai\",  # Use PhysicalAI's Observation format\n",
    ")\n",
    "\n",
    "# Create ACT policy directly from dataset (features extracted automatically)\n",
    "act_policy_from_dataset = ACT.from_dataset(\n",
    "    dataset=datamodule.train_dataset,\n",
    "    chunk_size=100,\n",
    "    dim_model=256,\n",
    "    n_encoder_layers=2,\n",
    "    n_decoder_layers=1,\n",
    ")\n",
    "act_policy_from_dataset.to(device)\n",
    "act_policy_from_dataset.eval()\n",
    "\n",
    "print(f\"âœ… ACT.from_dataset() created policy on {device}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in act_policy_from_dataset.parameters()):,}\")\n",
    "print(\"\\nðŸ“Š Features extracted automatically from dataset:\")\n",
    "print(f\"   Observation keys: {list(datamodule.train_dataset.observation_features.keys())}\")\n",
    "print(f\"   Action keys: {list(datamodule.train_dataset.action_features.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Evaluation Tools\n",
    "\n",
    "The `physicalai.eval.rollout` module provides production-ready evaluation with:\n",
    "- Success tracking from environment info\n",
    "- FPS monitoring  \n",
    "- Frame collection for visualization\n",
    "- Chunked action handling (ACT policies)\n",
    "- Video recording support\n",
    "- Distributed evaluation support (TorchMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the production rollout function\n",
    "from physicalai.eval.rollout import rollout\n",
    "\n",
    "# Run single episode with frames for visualization\n",
    "print(\"Running policy evaluation with production rollout()...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = rollout(gym, act_policy, max_steps=30, seed=42, return_frames=True)\n",
    "\n",
    "print(f\"Episode Length: {result['episode_length']} steps\")\n",
    "print(f\"Sum Reward: {result['sum_reward'].item():.4f}\")\n",
    "print(f\"Success: {result['success'].item() if 'success' in result else 'N/A'}\")\n",
    "print(f\"FPS: {result['fps']:.1f}\")\n",
    "print(f\"Frames collected: {len(result['frames'])}\")\n",
    "\n",
    "# Store for visualization\n",
    "act_result = {\n",
    "    \"frames\": result[\"frames\"],\n",
    "    \"actions\": result[\"action\"].squeeze(1).cpu().numpy(),\n",
    "    \"sum_reward\": result[\"sum_reward\"].item(),\n",
    "    \"steps\": result[\"episode_length\"],\n",
    "    \"fps\": result[\"fps\"],\n",
    "    \"success\": result[\"success\"].item() if \"success\" in result else False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Rollout\n",
    "\n",
    "Extract frames from observations returned by rollout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ACT rollout trajectory\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "indices = np.linspace(0, len(act_result[\"frames\"]) - 1, 5, dtype=int)\n",
    "for i, idx in enumerate(indices):\n",
    "    axes[i].imshow(act_result[\"frames\"][idx])\n",
    "    axes[i].set_title(f\"Step {idx}\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"ACT Policy Rollout\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize actions\n",
    "actions = act_result[\"actions\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(actions[:, :3])\n",
    "axes[0].legend([\"Î”x\", \"Î”y\", \"Î”z\"])\n",
    "axes[0].set_title(\"Position Actions\")\n",
    "axes[0].set_xlabel(\"Step\")\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(actions[:, 3:6])\n",
    "axes[1].plot(actions[:, 6], \"k--\", linewidth=2, label=\"gripper\")\n",
    "axes[1].legend([\"Î”roll\", \"Î”pitch\", \"Î”yaw\", \"gripper\"])\n",
    "axes[1].set_title(\"Rotation + Gripper Actions\")\n",
    "axes[1].set_xlabel(\"Step\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Third-Party Integration (LeRobot Policies)\n",
    "\n",
    "PhysicalAI also supports LeRobot policies (Diffusion, ACT, VQ-BeT, etc.) via `LeRobotPolicy` wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeRobot integration uses different feature format\n",
    "from lerobot.configs.types import FeatureType as LRFeatureType\n",
    "from lerobot.configs.types import PolicyFeature\n",
    "\n",
    "from physicalai.policies.lerobot import LeRobotPolicy\n",
    "\n",
    "# LeRobot-style features (note the different naming convention)\n",
    "lr_input_features = {\n",
    "    \"observation.images.image\": PolicyFeature(type=LRFeatureType.VISUAL, shape=(3, 256, 256)),\n",
    "    \"observation.images.image2\": PolicyFeature(type=LRFeatureType.VISUAL, shape=(3, 256, 256)),\n",
    "    \"observation.state\": PolicyFeature(type=LRFeatureType.STATE, shape=(8,)),\n",
    "}\n",
    "lr_output_features = {\n",
    "    \"action\": PolicyFeature(type=LRFeatureType.ACTION, shape=(7,)),\n",
    "}\n",
    "\n",
    "# Create LeRobot Diffusion policy\n",
    "# Pass config kwargs directly as **kwargs\n",
    "diffusion_policy = LeRobotPolicy(\n",
    "    policy_name=\"diffusion\",\n",
    "    input_features=lr_input_features,\n",
    "    output_features=lr_output_features,\n",
    "    crop_shape=None,  # Pass directly as kwarg\n",
    ")\n",
    "diffusion_policy.to(device)\n",
    "diffusion_policy.eval()\n",
    "\n",
    "print(f\"âœ“ LeRobot Diffusion policy created: {type(diffusion_policy._lerobot_policy).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare first-party ACT vs LeRobot Diffusion using production rollout\n",
    "print(\"Comparing policies with production rollout()...\")\n",
    "\n",
    "print(\"\\n1. First-party ACT:\")\n",
    "act_result = rollout(gym, act_policy, max_steps=20, seed=42, return_frames=True)\n",
    "print(f\"   Steps: {act_result['episode_length']}, Success: {act_result.get('success', 'N/A')}, FPS: {act_result['fps']:.1f}\")\n",
    "\n",
    "print(\"\\n2. LeRobot Diffusion:\")\n",
    "diffusion_result = rollout(gym, diffusion_policy, max_steps=20, seed=42, return_frames=True)\n",
    "print(f\"   Steps: {diffusion_result['episode_length']}, Success: {diffusion_result.get('success', 'N/A')}, FPS: {diffusion_result['fps']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison using frames from production rollout\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "# ACT trajectory (frames from rollout)\n",
    "act_frames = act_result[\"frames\"]\n",
    "indices = np.linspace(0, len(act_frames) - 1, 5, dtype=int)\n",
    "for i, idx in enumerate(indices):\n",
    "    axes[0, i].imshow(act_frames[idx])\n",
    "    axes[0, i].set_title(f\"Step {idx}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "axes[0, 0].set_ylabel(\"ACT\\n(first-party)\", fontsize=11)\n",
    "\n",
    "# Diffusion trajectory (frames from rollout)\n",
    "diff_frames = diffusion_result[\"frames\"]\n",
    "indices = np.linspace(0, len(diff_frames) - 1, 5, dtype=int)\n",
    "for i, idx in enumerate(indices):\n",
    "    axes[1, i].imshow(diff_frames[idx])\n",
    "    axes[1, i].set_title(f\"Step {idx}\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "axes[1, 0].set_ylabel(\"Diffusion\\n(LeRobot)\", fontsize=11)\n",
    "\n",
    "plt.suptitle(\"Policy Comparison: First-Party vs Third-Party (using rollout())\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate multi-episode evaluation with Rollout metric\n",
    "from physicalai.eval.rollout import Rollout, evaluate_policy\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Multi-Episode Evaluation with Rollout Metric\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Option 1: Using evaluate_policy function\n",
    "print(\"\\n1. evaluate_policy() - 3 episodes:\")\n",
    "eval_result = evaluate_policy(gym, act_policy, n_episodes=3, start_seed=0, max_steps=30)\n",
    "print(f\"   Avg Sum Reward: {eval_result['aggregated']['avg_sum_reward']:.4f}\")\n",
    "print(f\"   Avg Episode Length: {eval_result['aggregated']['avg_episode_length']:.1f}\")\n",
    "print(f\"   Avg FPS: {eval_result['aggregated']['avg_fps']:.1f}\")\n",
    "if \"pc_success\" in eval_result[\"aggregated\"]:\n",
    "    print(f\"   Success Rate: {eval_result['aggregated']['pc_success']:.1f}%\")\n",
    "\n",
    "# Option 2: Using Rollout metric (TorchMetrics-compatible)\n",
    "print(\"\\n2. Rollout Metric (TorchMetrics-compatible):\")\n",
    "rollout_metric = Rollout(max_steps=30)\n",
    "rollout_metric.to(device)\n",
    "\n",
    "# Run 3 episodes\n",
    "for ep in range(3):\n",
    "    rollout_metric.update(gym, act_policy, seed=ep)\n",
    "\n",
    "# Compute aggregated metrics\n",
    "metrics = rollout_metric.compute()\n",
    "print(f\"   Avg Sum Reward: {metrics['avg_sum_reward']:.4f}\")\n",
    "print(f\"   Success Rate: {metrics['pc_success']:.1f}%\")\n",
    "print(f\"   Episodes Evaluated: {metrics['n_episodes']}\")\n",
    "\n",
    "# Get per-episode breakdown\n",
    "per_ep = rollout_metric.get_per_episode_data()\n",
    "print(\"\\n   Per-Episode Results:\")\n",
    "for i, ep_data in enumerate(per_ep):\n",
    "    success_str = \"âœ“\" if ep_data.get(\"success\", False) else \"âœ—\"\n",
    "    print(f\"   Episode {i + 1}: reward={ep_data['sum_reward']:.4f}, success={success_str}\")\n",
    "\n",
    "# Cleanup\n",
    "gym.close()\n",
    "print(\"\\nâœ… Environment closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: LiberoBenchmark (High-Level Evaluation)\n",
    "\n",
    "The `LiberoBenchmark` class provides **standardized evaluation** across entire task suites:\n",
    "\n",
    "| Feature | LiberoGym | LiberoBenchmark |\n",
    "|---------|-----------|-----------------|\n",
    "| **Level** | Low-level | High-level |\n",
    "| **Scope** | Single task | Multiple tasks |\n",
    "| **Use Case** | Development, debugging | Paper results, comparison |\n",
    "| **Video Recording** | Manual | Built-in |\n",
    "| **Results Export** | Manual | JSON, CSV |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LiberoBenchmark automatically creates gyms for all tasks in a suite\n",
    "from physicalai.benchmark import LiberoBenchmark\n",
    "\n",
    "# Create benchmark for libero_spatial (10 tasks)\n",
    "# Limit to first 2 tasks for demo speed\n",
    "benchmark = LiberoBenchmark(\n",
    "    task_suite=\"libero_spatial\",\n",
    "    task_ids=[0, 1],  # Evaluate only tasks 0 and 1 (use None for all)\n",
    "    num_episodes=2,   # 2 episodes per task\n",
    "    max_steps=30,     # Short episodes for demo\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Benchmark: {benchmark}\")\n",
    "print(f\"Tasks: {len(benchmark.gyms)}\")\n",
    "print(f\"Episodes per task: {benchmark.num_episodes}\")\n",
    "print(f\"Total rollouts: {len(benchmark.gyms) * benchmark.num_episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Single Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the ACT policy for benchmark (need to match new gym specs)\n",
    "# First, get observation specs from benchmark gyms\n",
    "sample_gym = benchmark.gyms[0]\n",
    "sample_obs, _ = sample_gym.reset(seed=0)\n",
    "\n",
    "# Create features matching the benchmark gym\n",
    "benchmark_input_features = {\n",
    "    \"image\": Feature(\n",
    "        ftype=FeatureType.VISUAL,\n",
    "        shape=(3, 256, 256),\n",
    "        normalization_data=NormalizationParameters(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ),\n",
    "    \"image2\": Feature(\n",
    "        ftype=FeatureType.VISUAL,\n",
    "        shape=(3, 256, 256),\n",
    "        normalization_data=NormalizationParameters(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ),\n",
    "    \"state\": Feature(\n",
    "        ftype=FeatureType.STATE,\n",
    "        shape=(8,),\n",
    "        normalization_data=NormalizationParameters(mean=[0.0] * 8, std=[1.0] * 8),\n",
    "    ),\n",
    "}\n",
    "benchmark_output_features = {\n",
    "    \"action\": Feature(\n",
    "        ftype=FeatureType.ACTION,\n",
    "        shape=(7,),\n",
    "        normalization_data=NormalizationParameters(mean=[0.0] * 7, std=[1.0] * 7),\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Create policy for benchmarking\n",
    "# Note: n_action_steps must be <= chunk_size (defaults to chunk_size if not specified)\n",
    "benchmark_policy = ACT(\n",
    "    model=ACTModel(\n",
    "        input_features=benchmark_input_features,\n",
    "        output_features=benchmark_output_features,\n",
    "        chunk_size=100,  # Use 100 to match default n_action_steps\n",
    "        dim_model=256,\n",
    "        n_encoder_layers=2,\n",
    "        n_decoder_layers=1,\n",
    "    )\n",
    ")\n",
    "benchmark_policy.to(device)\n",
    "benchmark_policy.eval()\n",
    "print(f\"âœ… Policy ready for benchmarking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark evaluation\n",
    "print(\"Running benchmark evaluation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = benchmark.evaluate(benchmark_policy)\n",
    "\n",
    "# Print summary\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access detailed per-task results\n",
    "print(\"\\nPer-Task Breakdown:\")\n",
    "print(\"-\" * 60)\n",
    "for task_result in results.task_results:\n",
    "    print(f\"  {task_result.task_id}:\")\n",
    "    print(f\"    Task: {task_result.task_name[:50]}...\")\n",
    "    print(f\"    Success Rate: {task_result.success_rate:.1f}%\")\n",
    "    print(f\"    Avg Reward: {task_result.avg_reward:.4f}\")\n",
    "    print(f\"    Avg Steps: {task_result.avg_episode_length:.1f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create temporary directory for outputs\n",
    "output_dir = Path(tempfile.mkdtemp()) / \"results/benchmark\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export to JSON (includes metadata, per-task, per-episode data)\n",
    "json_path = output_dir / \"results.json\"\n",
    "results.to_json(json_path)\n",
    "print(f\"âœ… Exported to JSON: {json_path}\")\n",
    "\n",
    "# Export to CSV (per-task summary)\n",
    "csv_path = output_dir / \"results.csv\"\n",
    "results.to_csv(csv_path)\n",
    "print(f\"âœ… Exported to CSV: {csv_path}\")\n",
    "\n",
    "# View the JSON structure\n",
    "import json\n",
    "with open(json_path) as f:\n",
    "    data = json.load(f)\n",
    "print(f\"\\nJSON keys: {list(data.keys())}\")\n",
    "print(f\"Metadata: {data['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Multi-Policy Comparison\n",
    "\n",
    "To compare multiple policies, simply iterate over them with a dict comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second policy (same architecture, different instance)\n",
    "# In practice, these would be different models (ACT vs Diffusion vs VQ-BeT)\n",
    "policy_v1 = benchmark_policy\n",
    "policy_v1.name = \"ACT_v1\"  # Set a name for identification\n",
    "\n",
    "policy_v2 = ACT(\n",
    "    model=ACTModel(\n",
    "        input_features=benchmark_input_features,\n",
    "        output_features=benchmark_output_features,\n",
    "        chunk_size=100,  # Different chunk size\n",
    "        dim_model=128,   # Smaller model\n",
    "        n_encoder_layers=1,\n",
    "        n_decoder_layers=1,\n",
    "    )\n",
    ")\n",
    "policy_v2.name = \"ACT_v2_small\"\n",
    "policy_v2.to(device)\n",
    "policy_v2.eval()\n",
    "\n",
    "# Compare multiple policies using dict comprehension\n",
    "print(\"Comparing multiple policies...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "policies = [policy_v1, policy_v2]\n",
    "multi_results = {p.name: benchmark.evaluate(p) for p in policies}\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\nPolicy Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Policy':<20} {'Success Rate':>15} {'Avg Reward':>12}\")\n",
    "print(\"-\" * 60)\n",
    "for policy_name, result in multi_results.items():\n",
    "    print(f\"{policy_name:<20} {result.overall_success_rate:>14.1f}% {result.mean_reward:>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Video Recording with Benchmark\n",
    "\n",
    "`LiberoBenchmark` supports built-in video recording for debugging and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create benchmark with video recording enabled\n",
    "video_dir = output_dir / \"videos\"\n",
    "\n",
    "benchmark_with_video = LiberoBenchmark(\n",
    "    task_suite=\"libero_spatial\",\n",
    "    task_ids=[0],           # Single task for demo\n",
    "    num_episodes=2,\n",
    "    max_steps=30,\n",
    "    video_dir=video_dir,    # Enable video recording\n",
    "    record_mode=\"failures\", # Only save failed episodes (\"all\", \"failures\", \"successes\", \"none\")\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Video recording enabled to: {video_dir}\")\n",
    "print(f\"Record mode: failures (only save failed episodes)\")\n",
    "\n",
    "# Run evaluation with video recording\n",
    "results_with_video = benchmark_with_video.evaluate(benchmark_policy)\n",
    "\n",
    "# Check for saved videos\n",
    "import os\n",
    "if video_dir.exists():\n",
    "    videos = list(video_dir.rglob(\"*.mp4\"))\n",
    "    print(f\"\\nâœ… Videos saved: {len(videos)}\")\n",
    "    for v in videos:\n",
    "        print(f\"   - {v.relative_to(video_dir)}\")\n",
    "else:\n",
    "    print(\"\\n(No videos saved - all episodes may have succeeded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Using Explicit Gyms (Advanced)\n",
    "\n",
    "For custom configurations, use the base `Benchmark` class with explicit gyms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from physicalai.benchmark import Benchmark\n",
    "\n",
    "# Create gyms with custom configurations\n",
    "custom_gyms = [\n",
    "    LiberoGym(\n",
    "        task_suite=\"libero_spatial\",\n",
    "        task_id=i,\n",
    "        observation_height=128,  # Custom resolution\n",
    "        observation_width=128,\n",
    "        obs_type=\"pixels_agent_pos\",\n",
    "    )\n",
    "    for i in range(2)  # First 2 tasks\n",
    "]\n",
    "\n",
    "# Create benchmark with explicit gyms\n",
    "custom_benchmark = Benchmark(\n",
    "    gyms=custom_gyms,\n",
    "    num_episodes=2,\n",
    "    max_steps=30,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Custom benchmark: {custom_benchmark}\")\n",
    "print(f\"Gym observation size: {custom_gyms[0].observation_height}x{custom_gyms[0].observation_width}\")\n",
    "\n",
    "# Clean up custom gyms\n",
    "for g in custom_gyms:\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "**Part 1-2: LiberoGym (Low-Level)**\n",
    "- `LiberoGym` wraps LIBERO environments with Gymnasium interface\n",
    "- Observations include images (`image`, `image2`) and state (proprioception)\n",
    "- Supports multiple task suites (spatial, object, goal, 10, 90)\n",
    "\n",
    "**Part 3-4: Policy Creation & Evaluation**\n",
    "- First-party ACT via `ACT(model=ACTModel(...))`\n",
    "- Automatic feature extraction via `ACT.from_dataset()`\n",
    "- Third-party LeRobot policies via `LeRobotPolicy` wrapper\n",
    "- Production evaluation with `rollout()`, `Rollout`, and `evaluate_policy()`\n",
    "\n",
    "**Part 5: LiberoBenchmark (High-Level)**\n",
    "- `LiberoBenchmark` for standardized multi-task evaluation\n",
    "- Automatic gym creation from task suites\n",
    "- Built-in video recording\n",
    "- Export to JSON/CSV\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "# Low-level: Single task development\n",
    "from physicalai.gyms import LiberoGym\n",
    "gym = LiberoGym(task_suite=\"libero_10\", task_id=0)\n",
    "result = rollout(gym, policy, max_steps=300)\n",
    "\n",
    "# High-level: Full benchmark evaluation  \n",
    "from physicalai.benchmark import LiberoBenchmark\n",
    "benchmark = LiberoBenchmark(\n",
    "    task_suite=\"libero_10\",\n",
    "    num_episodes=20,\n",
    "    video_dir=\"./videos\",\n",
    "    record_mode=\"failures\",\n",
    ")\n",
    "results = benchmark.evaluate(policy)\n",
    "results.to_json(\"results.json\")\n",
    "\n",
    "# Compare multiple policies\n",
    "all_results = {p.name: benchmark.evaluate(p) for p in [policy1, policy2]}\n",
    "```\n",
    "\n",
    "### Task Suites Reference\n",
    "\n",
    "| Suite | Tasks | Focus | Recommended Episodes |\n",
    "|-------|-------|-------|---------------------|\n",
    "| `libero_spatial` | 10 | Spatial reasoning | 50 |\n",
    "| `libero_object` | 10 | Object generalization | 50 |\n",
    "| `libero_goal` | 10 | Goal-conditioned | 50 |\n",
    "| `libero_10` | 10 | Mixed difficulty | 50 |\n",
    "| `libero_90` | 90 | Comprehensive | 20 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import shutil\n",
    "\n",
    "# Close benchmark gyms\n",
    "for g in benchmark.gyms:\n",
    "    g.close()\n",
    "for g in benchmark_with_video.gyms:\n",
    "    g.close()\n",
    "\n",
    "# Remove temporary directory\n",
    "shutil.rmtree(output_dir.parent, ignore_errors=True)\n",
    "\n",
    "print(\"âœ… Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physicalai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
