{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBERO Benchmark Guide: Running GetiAction/LeRobot Policies\n",
    "\n",
    "This notebook provides a comprehensive guide to:\n",
    "1. Understanding the **LIBERO benchmark** for robot manipulation\n",
    "2. Using `LiberoGym` to interact with LIBERO environments\n",
    "3. Running **GetiAction/LeRobot policies** (Diffusion, ACT) for evaluation\n",
    "\n",
    "## What is LIBERO?\n",
    "\n",
    "**LIBERO** (Lifelong robotic learning BEnchmaRk with knOwledge transfer) is a benchmark designed for:\n",
    "- **Lifelong learning** in robotics\n",
    "- **Knowledge transfer** across tasks\n",
    "- **Standardized evaluation** of manipulation policies\n",
    "\n",
    "It provides **130 diverse manipulation tasks** across 5 task suites, using a simulated Franka Panda robot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# GetiAction imports\n",
    "from getiaction.data import Feature, FeatureType, NormalizationParameters\n",
    "from getiaction.devices import get_available_device\n",
    "from getiaction.gyms.libero import LiberoGym\n",
    "from getiaction.policies import ACT, ACTModel\n",
    "\n",
    "# Check device (supports CUDA, XPU, and CPU)\n",
    "device = get_available_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LIBERO Benchmark Overview\n",
    "\n",
    "| Suite | Tasks | Max Steps | Focus |\n",
    "|-------|-------|-----------|-------|\n",
    "| `libero_spatial` | 10 | 280 | Spatial reasoning (same objects, different positions) |\n",
    "| `libero_object` | 10 | 280 | Object generalization (different objects, same actions) |\n",
    "| `libero_goal` | 10 | 300 | Goal-conditioned tasks (same scene, different goals) |\n",
    "| `libero_10` | 10 | 520 | Mixed difficulty benchmark |\n",
    "| `libero_90` | 90 | 400 | Large-scale comprehensive benchmark |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from libero.libero import benchmark\n",
    "except ModuleNotFoundError:\n",
    "    msg = \"LIBERO is not installed. Install it with:\\n  pip install hf-libero\\nOr with uv:\\n  uv pip install hf-libero\"\n",
    "    raise ImportError(msg) from None\n",
    "\n",
    "# List available tasks\n",
    "for suite_name in [\"libero_spatial\", \"libero_object\"]:\n",
    "    suite = benchmark.get_benchmark_dict()[suite_name]()\n",
    "    tasks = suite.get_task_names()\n",
    "    print(f\"\\n{suite_name} ({len(tasks)} tasks):\")\n",
    "    for i, task in enumerate(tasks[:3]):\n",
    "        print(f\"  [{i}] {task}\")\n",
    "    if len(tasks) > 3:\n",
    "        print(f\"  ... and {len(tasks) - 3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create LiberoGym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LiberoGym instance\n",
    "gym = LiberoGym(\n",
    "    task_suite=\"libero_spatial\",\n",
    "    task_id=0,\n",
    "    observation_height=256,\n",
    "    observation_width=256,\n",
    "    obs_type=\"pixels_agent_pos\",  # Images + proprioception\n",
    "    control_mode=\"relative\",  # Delta actions\n",
    ")\n",
    "\n",
    "print(f\"Task: {gym.task_name}\")\n",
    "print(f\"Max episode steps: {gym.max_episode_steps}\")\n",
    "print(f\"Action space: {gym.action_space.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset and inspect observation\n",
    "obs, info = gym.reset(seed=42)\n",
    "\n",
    "print(f\"Observation type: {type(obs).__name__}\")\n",
    "print(f\"\\nImages: {list(obs.images.keys())}\")\n",
    "for name, img in obs.images.items():\n",
    "    print(f\"  {name}: {img.shape}\")\n",
    "\n",
    "print(f\"\\nState: {obs.state.shape}\")\n",
    "print(\"  Format: [eef_pos(3), axis_angle(3), gripper(2)]\")\n",
    "print(f\"  Values: {obs.state.squeeze().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize camera views\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "img1 = obs.images[\"image\"].squeeze(0).permute(1, 2, 0).numpy()\n",
    "img2 = obs.images[\"image2\"].squeeze(0).permute(1, 2, 0).numpy()\n",
    "\n",
    "axes[0].imshow(img1)\n",
    "axes[0].set_title(\"Front Camera (agentview)\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(img2)\n",
    "axes[1].set_title(\"Eye-in-Hand Camera\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Task: {gym.task_name[:50]}...\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Policy Features (Manual Approach)\n",
    "\n",
    "GetiAction uses `Feature` dataclasses to define input/output shapes. This is the **manual approach** - you can also use `ACT.from_dataset()` for automatic feature extraction (see Section 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features matching LiberoGym output (with normalization for ACT)\n",
    "input_features = {\n",
    "    \"image\": Feature(\n",
    "        ftype=FeatureType.VISUAL,\n",
    "        shape=(3, 256, 256),\n",
    "        normalization_data=NormalizationParameters(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ),\n",
    "    \"image2\": Feature(\n",
    "        ftype=FeatureType.VISUAL,\n",
    "        shape=(3, 256, 256),\n",
    "        normalization_data=NormalizationParameters(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ),\n",
    "    \"state\": Feature(\n",
    "        ftype=FeatureType.STATE,\n",
    "        shape=(8,),\n",
    "        normalization_data=NormalizationParameters(mean=[0.0] * 8, std=[1.0] * 8),\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Output features (7-dim action)\n",
    "output_features = {\n",
    "    \"action\": Feature(\n",
    "        ftype=FeatureType.ACTION,\n",
    "        shape=(7,),\n",
    "        normalization_data=NormalizationParameters(mean=[0.0] * 7, std=[1.0] * 7),\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"Input features:\")\n",
    "for name, feat in input_features.items():\n",
    "    print(f\"  {name}: {feat.ftype.name} {feat.shape}\")\n",
    "print(f\"\\nOutput: action {output_features['action'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create First-Party ACT Policy\n",
    "\n",
    "Two approaches to create an ACT policy:\n",
    "1. **Manual**: Define features yourself (shown above)\n",
    "2. **From Dataset**: Use `ACT.from_dataset()` for automatic feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Manual feature definition (from Section 4)\n",
    "act_model = ACTModel(\n",
    "    input_features=input_features,\n",
    "    output_features=output_features,\n",
    "    chunk_size=100,\n",
    "    dim_model=256,\n",
    "    n_encoder_layers=2,\n",
    "    n_decoder_layers=1,\n",
    ")\n",
    "act_policy = ACT(model=act_model)\n",
    "act_policy.to(device)\n",
    "act_policy.eval()\n",
    "\n",
    "print(f\"âœ… First-party ACT policy created on {device}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in act_policy.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Using `ACT.from_dataset()` (Recommended)\n",
    "\n",
    "This approach automatically extracts features from a LIBERO dataset on HuggingFace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Create ACT from a LeRobot dataset (automatic feature extraction)\n",
    "# Note: LIBERO datasets on HuggingFace may need format migration for LeRobot v3.0\n",
    "# Here we demonstrate with a compatible ALOHA dataset - same pattern works for LIBERO\n",
    "\n",
    "from getiaction.data.lerobot import LeRobotDataModule\n",
    "\n",
    "# Create datamodule from a compatible LeRobot dataset\n",
    "# For LIBERO, use \"lerobot/libero_spatial\" once migrated to v3.0 format\n",
    "datamodule = LeRobotDataModule(\n",
    "    repo_id=\"lerobot/aloha_sim_transfer_cube_human\",  # Example with ALOHA (2 arms, 4 cameras)\n",
    "    train_batch_size=32,\n",
    "    data_format=\"getiaction\",  # Use GetiAction's Observation format\n",
    ")\n",
    "\n",
    "# Create ACT policy directly from dataset (features extracted automatically!)\n",
    "act_policy_from_dataset = ACT.from_dataset(\n",
    "    dataset=datamodule.train_dataset,\n",
    "    chunk_size=100,\n",
    "    dim_model=256,\n",
    "    n_encoder_layers=2,\n",
    "    n_decoder_layers=1,\n",
    ")\n",
    "act_policy_from_dataset.to(device)\n",
    "act_policy_from_dataset.eval()\n",
    "\n",
    "print(f\"âœ… ACT.from_dataset() created policy on {device}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in act_policy_from_dataset.parameters()):,}\")\n",
    "print(\"\\nðŸ“Š Features extracted automatically from dataset:\")\n",
    "print(f\"   Observation keys: {list(datamodule.train_dataset.observation_features.keys())}\")\n",
    "print(f\"   Action keys: {list(datamodule.train_dataset.action_features.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def run_episode(gym, policy, max_steps=50, seed=42):\n",
    "    \"\"\"Run a single episode with a policy.\n",
    "\n",
    "    Handles both chunked (ACT) and non-chunked (Diffusion) policies.\n",
    "    \"\"\"\n",
    "    obs, _ = gym.reset(seed=seed)\n",
    "    device = next(policy.parameters()).device\n",
    "    obs = obs.to(device)\n",
    "\n",
    "    frames, actions, rewards = [], [], []\n",
    "    start = time.time()\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        with torch.no_grad():\n",
    "            action = policy(obs)\n",
    "            # Handle chunked outputs (ACT returns [batch, chunk_size, action_dim])\n",
    "            if action.dim() == 3:\n",
    "                action = action[:, 0, :]\n",
    "\n",
    "        action_np = action.squeeze(0).cpu().numpy()\n",
    "        actions.append(action_np)\n",
    "\n",
    "        obs, reward, done, truncated, _ = gym.step(action_np)\n",
    "        obs = obs.to(device)\n",
    "        rewards.append(reward)\n",
    "        frames.append(obs.images[\"image\"].squeeze(0).permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"frames\": np.array(frames),\n",
    "        \"actions\": np.array(actions),\n",
    "        \"sum_reward\": sum(rewards),\n",
    "        \"steps\": len(frames),\n",
    "        \"fps\": len(frames) / (time.time() - start),\n",
    "        \"success\": gym.check_success(),\n",
    "    }\n",
    "\n",
    "\n",
    "# Run ACT policy\n",
    "print(\"Running first-party ACT policy...\")\n",
    "act_result = run_episode(gym, act_policy, max_steps=30)\n",
    "print(f\"  Steps: {act_result['steps']}, Success: {act_result['success']}, FPS: {act_result['fps']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Rollout\n",
    "\n",
    "Extract frames from observations returned by rollout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ACT rollout trajectory\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "indices = np.linspace(0, len(act_result[\"frames\"]) - 1, 5, dtype=int)\n",
    "for i, idx in enumerate(indices):\n",
    "    axes[i].imshow(act_result[\"frames\"][idx])\n",
    "    axes[i].set_title(f\"Step {idx}\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"ACT Policy Rollout\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize actions\n",
    "actions = act_result[\"actions\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(actions[:, :3])\n",
    "axes[0].legend([\"Î”x\", \"Î”y\", \"Î”z\"])\n",
    "axes[0].set_title(\"Position Actions\")\n",
    "axes[0].set_xlabel(\"Step\")\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(actions[:, 3:6])\n",
    "axes[1].plot(actions[:, 6], \"k--\", linewidth=2, label=\"gripper\")\n",
    "axes[1].legend([\"Î”roll\", \"Î”pitch\", \"Î”yaw\", \"gripper\"])\n",
    "axes[1].set_title(\"Rotation + Gripper Actions\")\n",
    "axes[1].set_xlabel(\"Step\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Third-Party Integration: LeRobot Policies\n",
    "\n",
    "GetiAction also supports LeRobot policies (Diffusion, ACT, VQ-BeT, etc.) via `LeRobotPolicy` wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeRobot integration uses different feature format\n",
    "from lerobot.configs.types import FeatureType as LRFeatureType\n",
    "from lerobot.configs.types import PolicyFeature\n",
    "\n",
    "from getiaction.policies.lerobot import LeRobotPolicy\n",
    "\n",
    "# LeRobot-style features (note the different naming convention)\n",
    "lr_input_features = {\n",
    "    \"observation.images.image\": PolicyFeature(type=LRFeatureType.VISUAL, shape=(3, 256, 256)),\n",
    "    \"observation.images.image2\": PolicyFeature(type=LRFeatureType.VISUAL, shape=(3, 256, 256)),\n",
    "    \"observation.state\": PolicyFeature(type=LRFeatureType.STATE, shape=(8,)),\n",
    "}\n",
    "lr_output_features = {\n",
    "    \"action\": PolicyFeature(type=LRFeatureType.ACTION, shape=(7,)),\n",
    "}\n",
    "\n",
    "# Create LeRobot Diffusion policy\n",
    "diffusion_policy = LeRobotPolicy(\n",
    "    policy_name=\"diffusion\",\n",
    "    input_features=lr_input_features,\n",
    "    output_features=lr_output_features,\n",
    "    config_kwargs={\"crop_shape\": None},\n",
    ")\n",
    "diffusion_policy.to(device)\n",
    "diffusion_policy.eval()\n",
    "\n",
    "print(f\"âœ… LeRobot Diffusion policy created on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare first-party ACT vs LeRobot Diffusion\n",
    "print(\"Comparing policies...\")\n",
    "\n",
    "print(\"\\n1. First-party ACT:\")\n",
    "act_result = run_episode(gym, act_policy, max_steps=20)\n",
    "print(f\"   Steps: {act_result['steps']}, Success: {act_result['success']}, FPS: {act_result['fps']:.1f}\")\n",
    "\n",
    "print(\"\\n2. LeRobot Diffusion:\")\n",
    "diffusion_result = run_episode(gym, diffusion_policy, max_steps=20)\n",
    "print(\n",
    "    f\"   Steps: {diffusion_result['steps']}, Success: {diffusion_result['success']}, FPS: {diffusion_result['fps']:.1f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "# ACT trajectory\n",
    "indices = np.linspace(0, len(act_result[\"frames\"]) - 1, 5, dtype=int)\n",
    "for i, idx in enumerate(indices):\n",
    "    axes[0, i].imshow(act_result[\"frames\"][idx])\n",
    "    axes[0, i].set_title(f\"Step {idx}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "axes[0, 0].set_ylabel(\"ACT\\n(first-party)\", fontsize=11)\n",
    "\n",
    "# Diffusion trajectory\n",
    "indices = np.linspace(0, len(diffusion_result[\"frames\"]) - 1, 5, dtype=int)\n",
    "for i, idx in enumerate(indices):\n",
    "    axes[1, i].imshow(diffusion_result[\"frames\"][idx])\n",
    "    axes[1, i].set_title(f\"Step {idx}\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "axes[1, 0].set_ylabel(\"Diffusion\\n(LeRobot)\", fontsize=11)\n",
    "\n",
    "plt.suptitle(\"Policy Comparison: First-Party vs Third-Party\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "gym.close()\n",
    "print(\"âœ… Environment closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Learned:\n",
    "- **LIBERO**: 5 task suites, 130 manipulation tasks\n",
    "- **LiberoGym**: Gymnasium wrapper with compatible observations\n",
    "- **First-party ACT**: Native GetiAction implementation\n",
    "- **Third-party**: LeRobot policies (Diffusion, ACT, VQ-BeT) via wrapper\n",
    "\n",
    "\n",
    "### Compatible LeRobot Datasets (v3.0):\n",
    "| Dataset | Description |\n",
    "|---------|-------------|\n",
    "| `lerobot/aloha_sim_transfer_cube_human` | ALOHA bimanual cube transfer |\n",
    "| `lerobot/pusht` | 2D pushing task |\n",
    "| `lerobot/aloha_mobile_*` | Mobile ALOHA tasks |\n",
    "\n",
    "> **Note**: Some LIBERO datasets on HuggingFace use older formats. Check [LeRobot docs](https://huggingface.co/lerobot) for dataset migration.\n",
    "\n",
    "### Next Steps:\n",
    "1. Train ACT on demonstrations using `ACT.from_dataset()`\n",
    "2. Load pretrained checkpoints from HuggingFace\n",
    "3. Run full benchmark evaluation with LiberoGym"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geti-action",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
