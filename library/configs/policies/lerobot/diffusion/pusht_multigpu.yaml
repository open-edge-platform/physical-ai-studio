# Diffusion Policy - Multi-GPU Training Configuration
# Distributed training across multiple GPUs with DDP
#
# Usage:
#   getiaction fit --config configs/policies/lerobot/diffusion/pusht_multigpu.yaml

model:
  class_path: getiaction.policies.lerobot.diffusion.Diffusion
  init_args:
    # Temporal configuration
    n_obs_steps: 2
    horizon: 16
    n_action_steps: 8

    # Vision backbone
    vision_backbone: resnet50 # Larger model
    pretrained_backbone_weights: ResNet50_Weights.IMAGENET1K_V2
    crop_shape: [96, 96]
    crop_is_random: true
    use_group_norm: true # Better for distributed
    spatial_softmax_num_keypoints: 32

    # Large U-Net
    down_dims: [512, 1024, 2048]
    kernel_size: 5
    n_groups: 8
    diffusion_step_embed_dim: 256
    use_film_scale_modulation: true

    # Diffusion process
    noise_scheduler_type: DDPM
    num_train_timesteps: 100
    num_inference_steps: 10
    beta_schedule: squaredcos_cap_v2
    prediction_type: epsilon
    clip_sample: true

    # Optimization - scaled for multi-GPU
    learning_rate: 2.0e-4 # Scaled for batch_size * num_gpus
    optimizer_betas: [0.95, 0.999]
    optimizer_weight_decay: 1.0e-6
    scheduler_name: cosine
    scheduler_warmup_steps: 2000 # Longer warmup

data:
  class_path: getiaction.data.lerobot.LeRobotDataModule
  init_args:
    repo_id: lerobot/pusht
    train_batch_size: 64 # Per-GPU batch size
    data_format: lerobot
    delta_timestamps:
      action: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5]

trainer:
  max_epochs: 3000
  accelerator: gpu
  devices: 4 # Use 4 GPUs
  strategy: ddp # Distributed Data Parallel
  precision: 16-mixed
  gradient_clip_val: 10.0
  accumulate_grad_batches: 2 # Effective batch: 64*4*2=512
  log_every_n_steps: 100
  check_val_every_n_epoch: 50
  sync_batchnorm: true # Sync across GPUs
