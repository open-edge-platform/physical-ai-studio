# =============================================================================
# Groot (GR00T-N1.5-3B) Policy Training Configuration
# First-party implementation with PyTorch native SDPA attention
# =============================================================================
#
# Default settings optimized for 24GB GPUs (RTX 3090, RTX 4090).
#
# Usage:
#   physicalai fit --config configs/physicalai/groot.yaml
#
# Override examples:
#   # Intel XPU
#   physicalai fit --config configs/physicalai/groot.yaml \
#       --trainer.accelerator xpu
#
#   # Change dataset
#   physicalai fit --config configs/physicalai/groot.yaml \
#       --data.repo_id lerobot/pusht
#
# =============================================================================
# MEMORY REQUIREMENTS (approximate)
# =============================================================================
#
# | Configuration                    | Trainable Params | VRAM Required |
# |----------------------------------|------------------|---------------|
# | tune_projector only (default)    | ~518M            | 24GB          |
# | tune_projector + tune_diffusion  | ~1.1B            | 48GB          |
#
# =============================================================================

model:
  class_path: physicalai.policies.groot.Groot
  init_args:
    # -------------------------------------------------------------------------
    # Model Architecture
    # -------------------------------------------------------------------------
    chunk_size: 50 # Number of action predictions per forward pass
    n_action_steps: 50 # Number of action steps to execute
    max_state_dim: 64 # Maximum state dimension (shorter states zero-padded)
    max_action_dim: 32 # Maximum action dimension (shorter actions zero-padded)

    # -------------------------------------------------------------------------
    # Model Source
    # -------------------------------------------------------------------------
    base_model_path: "nvidia/GR00T-N1.5-3B"
    tokenizer_assets_repo: "lerobot/eagle2hg-processor-groot-n1p5"
    embodiment_tag: "new_embodiment"

    # -------------------------------------------------------------------------
    # Attention Implementation
    # -------------------------------------------------------------------------
    # Options:
    #   - "sdpa"            : PyTorch native SDPA (default, works on CUDA/XPU)
    #   - "flash_attention_2": Requires flash-attn CUDA package
    #   - "eager"           : Fallback Python implementation
    attn_implementation: "sdpa"

    # -------------------------------------------------------------------------
    # Fine-tuning Control
    # -------------------------------------------------------------------------
    # These control which parts of the model are trainable.
    # Freezing more components reduces memory usage.
    #
    # Recommended settings by GPU VRAM:
    #
    # 24GB (RTX 3090, RTX 4090) - DEFAULT:
    #   tune_projector: true
    #   tune_diffusion_model: false
    #
    # 48GB+ (A6000, A100, L40):
    #   tune_projector: true
    #   tune_diffusion_model: true
    #
    tune_llm: false # Fine-tune LLM backbone (~2B params)
    tune_visual: false # Fine-tune vision tower (~300M params)
    tune_projector: true # Fine-tune projector (~518M params)
    tune_diffusion_model: false # Fine-tune DiT action head (~550M params)

    # -------------------------------------------------------------------------
    # Optimizer
    # -------------------------------------------------------------------------
    learning_rate: 1.0e-4
    weight_decay: 1.0e-5

    # -------------------------------------------------------------------------
    # Precision
    # -------------------------------------------------------------------------
    use_bf16: true # Use bfloat16 for memory efficiency

data:
  class_path: physicalai.data.lerobot.LeRobotDataModule
  init_args:
    # -------------------------------------------------------------------------
    # Dataset
    # -------------------------------------------------------------------------
    repo_id: "lerobot/aloha_sim_transfer_cube_human"
    data_format: "lerobot"

    # -------------------------------------------------------------------------
    # Batch Size
    # -------------------------------------------------------------------------
    # Recommended settings by GPU VRAM:
    #   - 24GB: train_batch_size: 1 (DEFAULT)
    #   - 48GB: train_batch_size: 2
    #
    # Use accumulate_grad_batches to achieve larger effective batch sizes
    train_batch_size: 1

trainer:
  # ---------------------------------------------------------------------------
  # Training Control
  # ---------------------------------------------------------------------------
  max_epochs: 100

  # ---------------------------------------------------------------------------
  # Gradient Accumulation
  # ---------------------------------------------------------------------------
  # Effective batch size = train_batch_size × accumulate_grad_batches
  #
  # Examples:
  #   - batch_size=1, accumulate=16 → effective batch = 16 (for 24GB GPU, DEFAULT)
  #   - batch_size=2, accumulate=8  → effective batch = 16 (for 48GB GPU)
  #   - batch_size=4, accumulate=4  → effective batch = 16 (for 80GB GPU)
  #
  accumulate_grad_batches: 16
  gradient_clip_val: 1.0

  # ---------------------------------------------------------------------------
  # Hardware
  # ---------------------------------------------------------------------------
  # accelerator options:
  #   - "gpu"  : NVIDIA CUDA GPUs (default)
  #   - "xpu"  : Intel GPUs
  #   - "auto" : Auto-detect
  #
  accelerator: gpu
  devices: 1

  # ---------------------------------------------------------------------------
  # Precision
  # ---------------------------------------------------------------------------
  # Options:
  #   - "bf16-mixed" : BFloat16 mixed precision (recommended for Ampere+)
  #   - "16-mixed"   : Float16 mixed precision (for older GPUs)
  #   - "32"         : Full precision (highest memory usage)
  #
  precision: bf16-mixed

  # ---------------------------------------------------------------------------
  # Logging & Checkpointing
  # ---------------------------------------------------------------------------
  log_every_n_steps: 10
  check_val_every_n_epoch: 10
  experiment_name: "groot_finetune"
# =============================================================================
# QUICK REFERENCE: GPU-SPECIFIC OVERRIDES
# =============================================================================
#
# Default config is optimized for 24GB GPUs.
#
# Intel XPU:
#   physicalai fit --config configs/physicalai/groot.yaml \
#       --trainer.accelerator xpu
#
# 48GB GPU - enable DiT fine-tuning:
#   physicalai fit --config configs/physicalai/groot.yaml \
#       --model.tune_diffusion_model true \
#       --data.train_batch_size 2 \
#       --trainer.accumulate_grad_batches 8
#
# =============================================================================
