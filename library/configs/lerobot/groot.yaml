# Groot (GR00T-N1.5) Policy Training Configuration
# Train NVIDIA's GR00T-N1.5-3B foundation model using LeRobot's implementation
#
# Usage:
#   getiaction fit --config configs/lerobot/groot.yaml
#
# Requirements:
#   - pip install getiaction[groot]
#   - Significant GPU memory (>24GB recommended with bf16)
#
# Override parameters:
#   getiaction fit --config configs/lerobot/groot.yaml --model.learning_rate 1e-5

model:
  class_path: getiaction.policies.lerobot.Groot
  init_args:
    # Action chunking
    chunk_size: 50
    n_action_steps: 50
    n_obs_steps: 1

    # Dimension limits
    max_state_dim: 64
    max_action_dim: 32

    # Image preprocessing
    image_size: [224, 224]

    # Base model
    base_model_path: "nvidia/GR00T-N1.5-3B"
    tokenizer_assets_repo: "lerobot/eagle2hg-processor-groot-n1p5"
    embodiment_tag: "new_embodiment"

    # Fine-tuning control - freeze backbone for efficiency
    tune_llm: false
    tune_visual: false
    tune_projector: true
    tune_diffusion_model: true

    # LoRA (optional, set lora_rank > 0 to enable)
    lora_rank: 0 # Set to 16 or 32 to enable LoRA
    lora_alpha: 16
    lora_dropout: 0.1
    lora_full_model: false

    # Optimization
    learning_rate: 1.0e-4
    optimizer_betas: [0.95, 0.999]
    optimizer_eps: 1.0e-8
    optimizer_weight_decay: 1.0e-5
    warmup_ratio: 0.05

    # Precision
    use_bf16: true

data:
  class_path: getiaction.data.lerobot.LeRobotDataModule
  init_args:
    repo_id: "lerobot/aloha_sim_transfer_cube_human"
    train_batch_size: 4 # Reduced for memory efficiency
    data_format: "lerobot"
    delta_timestamps:
      action:
        [
          0.0,
          0.02,
          0.04,
          0.06,
          0.08,
          0.1,
          0.12,
          0.14,
          0.16,
          0.18,
          0.2,
          0.22,
          0.24,
          0.26,
          0.28,
          0.3,
          0.32,
          0.34,
          0.36,
          0.38,
          0.4,
          0.42,
          0.44,
          0.46,
          0.48,
          0.5,
          0.52,
          0.54,
          0.56,
          0.58,
          0.6,
          0.62,
          0.64,
          0.66,
          0.68,
          0.7,
          0.72,
          0.74,
          0.76,
          0.78,
          0.8,
          0.82,
          0.84,
          0.86,
          0.88,
          0.9,
          0.92,
          0.94,
          0.96,
          0.98,
        ]

trainer:
  max_epochs: 100
  accelerator: gpu
  devices: 1
  precision: bf16-mixed
  log_every_n_steps: 10
  check_val_every_n_epoch: 1
