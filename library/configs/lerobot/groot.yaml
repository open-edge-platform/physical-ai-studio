# Groot (GR00T-N1.5) Policy Training Configuration
# Train NVIDIA's GR00T-N1.5-3B foundation model using LeRobot's implementation
#
# Usage:
#   getiaction fit --config configs/lerobot/groot.yaml
#
# Requirements:
#   - pip install getiaction[groot]
#   - Significant GPU memory (>24GB recommended with bf16)
#
# Override parameters:
#   getiaction fit --config configs/lerobot/groot.yaml --model.learning_rate 1e-5

model:
  class_path: getiaction.policies.lerobot.Groot
  init_args:
    # Action chunking
    chunk_size: 50
    n_action_steps: 50
    n_obs_steps: 1

    # Dimension limits
    max_state_dim: 64
    max_action_dim: 32

    # Image preprocessing
    image_size: [224, 224]

    # Base model
    base_model_path: "nvidia/GR00T-N1.5-3B"
    tokenizer_assets_repo: "lerobot/eagle2hg-processor-groot-n1p5"
    embodiment_tag: "new_embodiment"

    # Fine-tuning control - freeze backbone for efficiency
    tune_llm: false
    tune_visual: false
    tune_projector: true
    tune_diffusion_model: true

    # LoRA (optional, set lora_rank > 0 to enable)
    lora_rank: 0 # Set to 16 or 32 to enable LoRA
    lora_alpha: 16
    lora_dropout: 0.1
    lora_full_model: false

    # Optimization
    optimizer_lr: 1.0e-4
    optimizer_betas: [0.95, 0.999]
    optimizer_eps: 1.0e-8
    optimizer_weight_decay: 1.0e-5
    warmup_ratio: 0.05

    # Precision
    use_bf16: true

data:
  class_path: getiaction.data.lerobot.LeRobotDataModule
  init_args:
    repo_id: "lerobot/aloha_sim_transfer_cube_human"
    train_batch_size: 4 # Reduced for memory efficiency
    data_format: "lerobot"

trainer:
  max_epochs: 100
  accelerator: gpu
  devices: 1
  precision: bf16-mixed
  log_every_n_steps: 10
  check_val_every_n_epoch: 1
