# LeRobot Diffusion Policy Configuration
# Using Universal Wrapper Approach
#
# This config demonstrates how to use the universal LeRobotPolicy wrapper
# to instantiate a Diffusion Policy without an explicit wrapper.

model:
  class_path: getiaction.policies.lerobot.LeRobotPolicy
  init_args:
    policy_name: diffusion
    learning_rate: 1e-4
    # Policy-specific parameters must be nested under config_kwargs for Lightning CLI validation
    # See lerobot.configs.policies.diffusion.DiffusionConfig for all available parameters
    config_kwargs:
      horizon: 16 # Must match len(delta_timestamps["action"]) and work with downsampling. Default: 16
      n_action_steps: 16 # Number of action steps to predict. Default: 8
      num_train_timesteps: 100 # Number of diffusion timesteps during training. Default: 100
      noise_scheduler_type: DDPM # Type of noise scheduler ('DDPM' or 'DDIM'). Default: "DDPM"
      down_dims: [512, 1024, 2048] # Downsampling dimensions for the U-Net. Default: [512, 1024, 2048]

data:
  class_path: getiaction.data.lerobot.LeRobotDataModule
  init_args:
    repo_id: "lerobot/aloha_sim_transfer_cube_human"
    train_batch_size: 8
    data_format: "lerobot" # Output format: "lerobot" (flattened dict) or "getiaction" (Observation)
    delta_timestamps:
      action: [
          0.0,
          0.1,
          0.2,
          0.3,
          0.4,
          0.5,
          0.6,
          0.7,
          0.8,
          0.9,
          1.0,
          1.1,
          1.2,
          1.3,
          1.4,
          1.5,
        ] # 16 steps

trainer:
  max_epochs: 100
  precision: 16-mixed
  log_every_n_steps: 10
