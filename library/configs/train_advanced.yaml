# Example: Advanced training config with all features
# Shows mixed dataclass + class_path patterns

seed_everything: 42

trainer:
  max_epochs: 200
  accelerator: gpu
  devices: 1
  precision: 16-mixed  # Mixed precision training
  log_every_n_steps: 10
  gradient_clip_val: 1.0
  accumulate_grad_batches: 2
  check_val_every_n_epoch: 5

  # Callbacks
  callbacks:
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        patience: 15
        monitor: train/loss
        mode: min
        min_delta: 0.001

    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: checkpoints/
        monitor: train/loss
        mode: min
        save_top_k: 5
        filename: "epoch={epoch:03d}-loss={train/loss:.4f}"
        auto_insert_metric_name: false

    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step

    - class_path: lightning.pytorch.callbacks.RichProgressBar

  # Logger
  logger:
    - class_path: lightning.pytorch.loggers.TensorBoardLogger
      init_args:
        save_dir: logs/
        name: dummy_policy

    - class_path: lightning.pytorch.loggers.CSVLogger
      init_args:
        save_dir: logs/
        name: dummy_policy

# Model/Policy
model:
  class_path: getiaction.policies.dummy.policy.Dummy
  init_args:
    model:
      class_path: getiaction.policies.dummy.model.Dummy
      init_args:
        action_shape: [7]
        n_action_steps: 4
        temporal_ensemble_coeff: 0.1
        n_obs_steps: 2
        horizon: 8

    # optimizer:
    #   # You can use any PyTorch optimizer!
    #   class_path: torch.optim.AdamW
    #   init_args:
    #     lr: 0.001
    #     weight_decay: 0.00001
    #     betas: [0.9, 0.999]
    #     eps: 1.0e-08

# Data
data:
  class_path: getiaction.data.lerobot.LeRobotDataModule
  init_args:
    repo_id: "lerobot/pusht"
    train_batch_size: 32
    download_videos: true
    video_backend: "pyav"
    force_cache_sync: false

# Optimizer configuration (if using Lightning's optimizer config)
# optimizer:
#   class_path: torch.optim.AdamW
#   init_args:
#     lr: 0.001
#     weight_decay: 0.00001

# LR Scheduler (optional)
# lr_scheduler:
#   class_path: torch.optim.lr_scheduler.CosineAnnealingLR
#   init_args:
#     T_max: 200
#     eta_min: 0.00001

